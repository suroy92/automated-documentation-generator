# Automated Documentation Generator (Local â€” Ollama)

A robust, language-agnostic CLI that scans a project and generates documentation from source code using a Language-Agnostic Document Object Model (LADOM).  
**Runs entirely on your machine using [Ollama](https://ollama.com/). No API keys, no data leaving your device.**

---

## âœ¨ Features

- ğŸ” **Multi-language support**: Python & JavaScript (extensible analyzers)
- ğŸ§  **Local LLM**: Uses an Ollama model (default: `qwen2.5-coder:7b`)
- ğŸ§± **LADOM**: A consistent, language-agnostic schema for docs
- âš¡ **Parallel processing**: Multi-threaded scanning
- ğŸ’¾ **Smart caching**: Avoids regenerating docstrings
- ğŸ§° **Multiple outputs**: Markdown & HTML
- ğŸ” **Security-first**: Path validation & forbidden paths
- âš™ï¸ **Configurable**: YAML config for model, temperature, rate limits, etc.

---

## ğŸš€ Quick Start

### 1) Prerequisites
- **Python** 3.8+
- **Ollama** installed and running locally  
  One-time model pull:
  ```bash
  ollama pull qwen2.5-coder:7b
  ```

**Windows tip:** Keep large model files on a fast NVMe drive
Open PowerShell and set:
```powershell
$env:OLLAMA_MODELS = 'D:\ollama\models'
```
Restart the shell and (re)pull models if needed.

### 2) Install

```bash
git clone <your-repo-url>
cd automated-doc-generator
pip install -r requirements.txt
```

No `.env` or API keys required.

### 3) Run

```bash
python -m src.main
```

Youâ€™ll be prompted for the project path to scan. Output is written under `Documentation/` by default.

---

## âš™ï¸ Configuration

Edit **`config.yml`** at the repo root:

```yaml
# Directories to exclude from scanning
exclude_dirs:
  - node_modules
  - __pycache__
  - .git

# Output configuration
output:
  directory: Documentation
  format: markdown

# Local LLM configuration
llm:
  model: qwen2.5-coder:7b        # pull via: ollama pull qwen2.5-coder:7b
  base_url: http://localhost:11434
  temperature: 0.2
  rate_limit_calls_per_minute: 20
  embedding_model: all-minilm:l6-v2   # optional, for future embeddings use
  timeout_seconds: 120

# Caching
cache:
  enabled: true
  file: .docstring_cache.json

# Processing options
processing:
  parallel: true
  max_workers: 4

# Security hardening â€” keep secrets out of prompts (even locally)
security:
  forbidden_paths:
    - "**/.env"
    - "**/secrets/**"
    - "**/*.pem"
    - "**/*.key"
    - "**/.git/**"
    - "**/__pycache__/**"
    - "**/node_modules/**"
```

**Optional environment overrides**

* `OLLAMA_BASE_URL` (default `http://localhost:11434`)
* `DOCGEN_MODEL` (e.g., `qwen2.5-coder:7b`)
* `OLLAMA_TEMPERATURE`
* `DOCGEN_EMBED_MODEL`
* `DOCGEN_TIMEOUT`

---

## ğŸ§­ Project Structure

```
automated-doc-generator/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                  # Main entry point (now initializes local Ollama client)
â”‚   â”œâ”€â”€ config_loader.py         # Configuration management
â”‚   â”œâ”€â”€ ladom_schema.py          # LADOM schema & validation
â”‚   â”œâ”€â”€ cache_manager.py         # Docstring caching
â”‚   â”œâ”€â”€ rate_limiter.py          # Rate limiting
â”‚   â”œâ”€â”€ path_validator.py        # Path security checks
â”‚   â”œâ”€â”€ doc_generator.py         # Markdown/HTML generators
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â””â”€â”€ ollama_client.py     # NEW: local client for Ollama (no external deps)
â”‚   â””â”€â”€ analyzers/
â”‚       â”œâ”€â”€ base_analyzer.py     # calls client.generate(...)
â”‚       â”œâ”€â”€ py_analyzer.py       # Python analyzer
â”‚       â”œâ”€â”€ js_analyzer.py       # JavaScript analyzer
â”‚       â””â”€â”€ java_analyzer.py     # Java analyzer (optional)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ladom_schema.py
â”‚   â”œâ”€â”€ test_cache_manager.py
â”‚   â””â”€â”€ test_analyzers.py
â”œâ”€â”€ config.yaml                   # Configuration (local-first)
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

---

## ğŸ§ª Example Session

```text
$ python -m src.main
============================================================
  Automated Documentation Generator (Local â€“ Ollama)
============================================================

Enter the project path to scan: /path/to/your/project

Scanning project: /path/to/your/project
Found 23 files to analyze
Analyzing files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:19<00:00]

Generating documentation...

============================================================
  âœ“ Documentation generated successfully!
============================================================

Cache statistics:
  - Total entries: 18
  - Cache file: .docstring_cache.json

API calls made: 5
```

---

## ğŸ§± Architecture Overview

1. **File scanning** â†’ respects `exclude_dirs`
2. **Language analyzers** â†’ parse ASTs and extract symbols
3. **LADOM build** â†’ normalized, language-agnostic representation
4. **LLM docstrings** â†’ prompts a local model for concise descriptions
5. **Renderers** â†’ Markdown and HTML outputs

**LADOM (example)**

```json
{
  "project_name": "My Project",
  "files": [
    {
      "path": "/path/to/file.py",
      "functions": [
        {
          "name": "function_name",
          "description": "Function description",
          "parameters": [{"name":"param1","type":"str","description":"..."}],
          "returns": {"type":"int","description":"..."}
        }
      ],
      "classes": []
    }
  ]
}
```

---

## ğŸ› ï¸ Extending

### Add a new analyzer

Create `src/analyzers/my_lang_analyzer.py`:

```python
from .base_analyzer import BaseAnalyzer

class MyLanguageAnalyzer(BaseAnalyzer):
    def _get_language_name(self) -> str:
        return "mylanguage"

    def analyze(self, file_path: str):
        # Parse source; return LADOM-compliant dict
        ...
```

Register it in `src/main.py` to include files with your extension.

### Custom output formats

Add a generator in `src/doc_generator.py` and call it in `generate_documentation(...)`.

---

## ğŸ§© Troubleshooting

**â€œConnection refusedâ€ / timeouts**

* Ensure Ollama is running and reachable at `base_url`.
* Try: `curl http://localhost:11434/api/tags` (should list models).

**â€œmodel not foundâ€**

* Pull it first: `ollama pull qwen2.5-coder:7b`.

**Slow generations**

* Reduce context in prompts; keep to essential code snippets.
* Ensure only 1â€“2 concurrent *LLM* calls while scanning remains parallel.
* On Windows/NVIDIA, set â€œPrefer maximum performanceâ€ for Python in the NVIDIA Control Panel.

**Nothing analyzed**

* Confirm your file types are included and not excluded by `exclude_dirs`.

---

## ğŸ”’ Security

* All inference is local; nothing is sent to third-party services.
* `security.forbidden_paths` ensures secrets (e.g., `.env`, keys) are never read or sent.
* Cache file (`.docstring_cache.json`) is local and ignored by VCS (add to `.gitignore` if not already).

---

## ğŸ§­ Migration Note (from Gemini)

* The project no longer uses Googleâ€™s Gemini or API keys.
* Any previous references to `GEMINI_API_KEY` or `google.generativeai` have been removed in favor of a **local client** (`src/providers/ollama_client.py`).
* If you still have an old `.env` file, it is no longer used.

---

## ğŸ§ª Tests

```bash
pytest tests/ -v
# or with coverage
pytest tests/ --cov=src --cov-report=html
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a PR

---

## ğŸ“„ License

[LICENCE](./LICENSE)

---

## ğŸ™ Acknowledgments

* [Ollama](https://ollama.com/) for local model serving
* Python AST & Esprima for parsing
* Jinja2 for templating